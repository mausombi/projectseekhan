{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community==0.2.4 langchain==0.2.3 faiss-cpu==1.8.0 unstructured==0.14.5 unstructured[pdf]==0.14.5 transformers==4.41.2 sentence-transformers==3.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the LLM\n",
    "llm = Ollama(\n",
    "    model=\"laddo\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the document\n",
    "loader = UnstructuredFileLoader(\"PP Unit 2 Tesseract.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'PP Unit 2 Tesseract.pdf'}, page_content='UNIT-2\\n\\nTOPIC-1\\n\\nParallel Programming on CPU-I\\n\\nVectorization\\n\\nVectorization is a technique used in computer science to perform operations on entire arrays or sequences\\n\\nof data elements simultaneously, instead of processing each element individually. It\\'s commonly used in\\n\\nnumerical and scientific computing, as well as in various data analysis and machine learning tasks. In\\n\\nParallel computing, processors have special vector units that can load and operate on more than one data\\n\\nelement at a time.\\n\\nSIMD overview\\n\\nVectorization is an example of single instruction, multiple data (SIMD) processing because it executes a\\n\\nsingle operation (e.g., addition, division) over a large dataset. A scalar operation, in the context of\\n\\nmathematics and computer science, refers to an operation that is performed on a single scalar value, as\\n\\nopposed to a vector, matrix, or any other data structure. Scalars are single numerical values and can be\\n\\nintegers, floating-point numbers, or other numerical types.\\n\\nVectorization Terminology:\\n\\nVector (SIMD) lane : A pathway through a vector operation on vector registers for a single data element\\n\\nmuch like a lane on a multi-lane free way.\\n\\nVector width : The width of the vector unit, usually expressed in bits\\n\\nVector length :The number of data elements that can be processed by the vector in one operation.\\n\\nVector (SIMD) instruction sets:The set of instructions that extend the regular scalar processor\\n\\ninstructions to utilize the vector processor.\\n\\nVectorization is produced through both a software and a hardware component.\\n\\nThe requirements are\\n\\nGenerate Instruction The vector instructions must be the compiler or generated by through manually intrinsics or assembler codings\\n\\nspecified\\n\\nMatch instructions to the vector unit of the processor. If there is a mismatch between the instructions and the hardware, newer hardware can usually process\\n\\nHardware trends for vectorization\\n\\nIt is helpful to know the historical dates of hardware and instruction set release for selecting which\\n\\nvector instruction set to use.\\n\\nUNIT-2\\n\\nTOPIC-2\\n\\nParallel Programming on CPU-II\\n\\nVectorization methods\\n\\nThere are several ways to achieve Vectorization in your program.\\n\\nOptimized libraries :\\n\\nOptimized libraries play a crucial role in achieving vectorization and improving the performance of\\n\\nsoftware applications, especially in the context of numerical and scientific computing. These libraries\\n\\nprovide pre-implemented and highly optimized functions for common mathematical and linear\\n\\nalgebra operations. Some of the most commonly used libraries include\\n\\nBLAS (Basic Linear Algebra System)—A base component of high-performance linear algebra\\n\\nsoftware\\n\\nLAPACK—A linear algebra package\\n\\nSCALAPACK—A scalable linear algebra package\\n\\nFFT (Fast Fourier transform)—Various implementation packages available\\n\\nSparse Solvers—various implementations of sparse solvers available\\n\\nAuto-Vectorization\\n\\nAuto-vectorization is a compiler optimization technique that transforms scalar code into vectorized\\n\\ncode, taking advantage of SIMD (Single Instruction, Multiple Data) instructions available in modern\\n\\nprocessors. Most modern compilers provide flags or options to enable or enhance auto-vectorization.\\n\\nHints to the compiler\\n\\nHints to the compiler are annotations or directives provided by the programmer to guide the\\n\\ncompiler\\'s optimization decisions. These hints can inform the compiler about specific\\n\\noptimizations that should be applied to certain parts of the code. Different programming languages\\n\\nand compilers have different ways to provide hints to guide vectorization.\\n\\n1. Pragmas can guide the compiler\\'s vectorization process, helping it identify loops that can\\n\\nbe safely and efficiently vectorized.\\n\\n#pragma clang loop vectorize(enable) // Enable loop vectorization\\n\\nfor (int i = 0; i < N; ++i)\\n\\n{\\n\\n// Loop body\\n\\n}\\n\\n\\n\\n\\n\\n2.\\n\\nIn the context of computer programming and compiler optimizations dependencies play a\\n\\ncrucial role in determining the order in which instructions or operations can be executed.\\n\\nVarious data Depencies are\\n\\nA flow dependency, also known as a true dependency (Read After Write-RAW), occurs when an\\n\\ninstruction depends on the result of a previous instruction. As a result, the second instruction\\n\\ncannot be executed until the first one completes.\\n\\nFor example, if you have the code b = a + 1 and then c = b + 2, there is a flow dependency from\\n\\nthe second instruction to the first because it relies on the result of the first instruction.\\n\\nAn anti-flow dependency, also known as an anti-dependency(Write After Read-WAR), occurs\\n\\nwhen the order of execution of instructions is crucial to avoid incorrect results.\\n\\n1. b = a * 2 (Instruction 1)\\n\\n2. a = a + 1 (Instruction 2)\\n\\nIn this example, Instruction 2 modifies the value of the variable a, which is used in Instruction\\n\\n1. If Instruction 2 were to execute before Instruction 1, the value of a used in Instruction 1\\n\\nwould be incorrect, leading to erroneous results. Therefore, the correct order of execution is\\n\\nInstruction 1 followed by Instruction 2.\\n\\nAn output dependency, also known as a write-after-write dependency (WAW dependency), occurs\\n\\nwhen two instructions write to the same memory location or register.\\n\\nConsider the following sequence of instructions:\\n\\n1.\\n\\na = 5 (Instruction 1)\\n\\n2.\\n\\na = a + 3 (Instruction 2)\\n\\nIn this example, both Instruction 1 and Instruction 2 write to the same variable a. If Instruction\\n\\n1 and Instruction 2 are executed out of order, the final value of a depends on the order in which the\\n\\ninstructions are executed.\\n\\nIf Instruction 1 is executed after Instruction 2, a will be 5.\\n\\nIf Instruction 1 is executed before Instruction 2, a will be 8.\\n\\n3. Vectorization of Loops:\\n\\nA \"peel loop\" is a term used in the context of loop optimization in computer\\n\\nprogramming and compilers. Loop peeling refers to the process of extracting one or more\\n\\niterations from the beginning or end of a loop and handling them separately from the main\\n\\nloop.\\n\\n// Original loop\\n\\nfor (int i = 0; i < N; i++) {\\n\\n// Loop body\\n\\n}\\n\\n// Peeling the first iteration\\n\\n// Handle the first iteration separately(This can be beneficial, for example, if the first\\n\\niteration requires special processing, and the remaining iterations start from index 1.)\\n\\n// Main loop with iterations from 1 to N-1\\n\\nfor (int i = 1; i < N; i++) {\\n\\n// Loop body\\n\\n}\\n\\nA \"remainder loop\" refers to a loop that iterates over the remaining elements of a collection or\\n\\narray after a specific condition is met. It is a common programming construct used to process the\\n\\nremaining elements of a data structure once a certain criterion is satisfied within the loop.\\n\\nFor example, if the vectorized loop trip count is 20 and the vector length is 16, it means every time\\n\\nthe kernel loop gets executed once, the remainder 4 iterations have to be executed in the remainder-\\n\\nloop.\\n\\nThe peel loop is added to deal with the unaligned data at the start of the loop, and the remainder\\n\\nloop takes care of any extra data at the end of the loop\\n\\nVector intrinsic:\\n\\nVector intrinsics are low-level programming constructs used to write explicit vectorized code by\\n\\ndirectly utilizing the capabilities of SIMD (Single Instruction, Multiple Data) instructions\\n\\navailable on modern processors. Vector intrinsics are typically written in assembly or as inline\\n\\nassembly within a higher-level programming language and are often specific to a particular CPU\\n\\narchitecture.\\n\\nIntrisics provide data types for vectors (i.e. __m128 a; would declare the variable a to be a\\n\\nvector of 4 floats). They also provide functions\\n\\nthat operate directly on vectors\\n\\n(i.e. _mm_add_ps(a,b) would add together the two vectors a and b).\\n\\nAssembler instructions:\\n\\nUsing assembly language for vectorization involves writing low-level code that directly employs\\n\\nSIMD (Single Instruction, Multiple Data) instructions to perform operations on multiple data\\n\\nelements in parallel.\\n\\nThe example below demonstrates vectorization using x86 assembly with SSE (Streaming\\n\\nSIMD Extensions) instructions:\\n\\narray1 dd 1, 2, 3, 4 ; First array of integers\\n\\narray2 dd 5, 6, 7, 8 ; Second array of integers\\n\\nresult dd 0, 0, 0, 0 ; Array to store the result\\n\\n_start:\\n\\nmovaps xmm0, [array1] ; Load 128-bit (4x32-bit) values from array1 to xmm0\\n\\nmovaps xmm1, [array2] ; Load 128-bit (4x32-bit) values from array2 to xmm1\\n\\naddps xmm0, xmm1 ; Perform vectorized addition\\n\\nmovaps [result], xmm0 ; Store the result back to the result array\\n\\nExit\\n\\nProgramming style for better Vectorization:\\n\\nAdopting the following programming styles leads to better performance out of the box and less\\n\\nwork needed for optimization efforts.\\n\\nGeneral suggestions:\\n\\nUse the restrict attribute on pointers in function arguments and declarations (C and C++).\\n\\nUse pragmas or directives where needed to inform the compiler.\\n\\nBe careful with optimizing for the compiler with #pragma unroll and other techniques; you might\\n\\nlimit the possible options for the compiler transformations.\\n\\nPut exceptions and error checks with print statements in a separate loop.\\n\\nConcerning data structures:\\n\\nTry to use a data structure with a long length for the innermost loop\\n\\n\\n\\nUse the smallest data type needed (short rather than int).\\n\\nUse contiguous memory accesses.\\n\\nUse Structure of Arrays (SOA) rather than Array of Structures (AOS).\\n\\nArray of Structures (AoS)(structure variable is an array)\\n\\nstruct person {\\n\\nchar gender;\\n\\nint age;\\n\\n} s[5];\\n\\nStructure of Arrays (SoA)(Structure member is an array)\\n\\nstruct person {\\n\\nchar name[60];\\n\\nchar gender;\\n\\nint age;\\n\\n} ;\\n\\nUse memory-aligned data structures where possible.\\n\\nRelated to loop structures:\\n\\nUse simple loops without special exit conditions.\\n\\nMake loop bounds a local variable by copying global values and then using them.\\n\\nUse the loop index for array addresses when possible.\\n\\nExpose the loop bound size so it is known to the compiler. If the loop is only three iterations\\n\\nlong, the compiler might unroll the loop rather than generate a four-wide vector instruction.\\n\\nAvoid array syntax in performance-critical loops (FORTRAN).\\n\\nIn the loop body:\\n\\nDefine local variables within a loop so that it is clear that these are not carried to subsequent\\n\\niterations (C and C++).\\n\\nVariables and arrays within a loop should be write-only or read-only (only on the left side of the\\n\\nequal sign or on the right side, except for reductions).\\n\\nDon’t reuse local variables for a different purpose in the loop—create a new variable. The\\n\\nmemory space you waste is far less important than the confusion this creates for the compiler.\\n\\nConcerning compiler settings and flags:\\n\\n\\n\\nUse the latest version of a compiler and prefer compilers that do better\\n\\nvectorization.\\n\\n\\n\\nUse a strict aliasing compiler flag.\\n\\n\\n\\nGenerate code for the most powerful vector instruction set you can get away with\\n\\nUNIT-2\\n\\nTOPIC 3\\n\\nThe basics for an MPI program\\n\\nMPI (Message Passing Interface) is a standard communication protocol used in parallel computing\\n\\nenvironments to enable processes running on different processors or nodes to communicate and\\n\\ncoordinate their actions. It is commonly used in high-performance computing (HPC) and cluster\\n\\ncomputing applications where multiple computing nodes work together to solve a complex problem.\\n\\nMPI allows programs to be written in a distributed-memory programming model, where each process\\n\\nhas its own local memory space and communicates with other processes using message passing.\\n\\nProcesses can send and receive messages, making it possible for them to exchange data and\\n\\nsynchronize their execution.\\n\\nIn the context of MPI (Message Passing Interface) programming, the terms \"communication world\"\\n\\nand \"rank\" are fundamental concepts used to manage communication and coordination among\\n\\nparallel processes in a parallel computing environment.\\n\\n1. Communication World:\\n\\nA communication world, also known as a communicator, is a group of MPI processes that can\\n\\ncommunicate with each other.\\n\\nMPI_COMM_WORLD is the default communicator that includes all processes created when\\n\\nthe MPI application starts.\\n\\nCommunicators allow processes to be organized into groups, enabling more controlled and\\n\\nspecific communication patterns.\\n\\n2. Rank:\\n\\nRank refers to the unique identifier assigned to each process within a communicator.\\n\\n\\n\\nIn MPI_COMM_WORLD, ranks range from 0 to (number of processes - 1).\\n\\nRanks are used to distinguish one process from another within the same communicator.\\n\\nProcesses can communicate with each other using their ranks as identifiers.\\n\\nThe diagram shows a program which runs with five processes. In this example, the size of\\n\\nMPI_COMM_WORLD is 5. The rank of each process is the number inside each circle. The rank of\\n\\na process always ranges from 0 to 4.\\n\\nMPI Functions\\n\\n1.\\n\\nMPI_Comm_rank(MPI_COMM_WORLD, &rank) : The rank of a process within a\\n\\ncommunicator can be obtained using this\\n\\nParameters\\n\\nMPI_COMM_WORLD: This is a predefined communicator in MPI that includes all processes\\n\\nspawned by the MPI program. It is a communicator for the world of all processes.\\n\\n&rank: This is the address of the variable where the rank of the calling process will be stored.\\n\\nThe function retrieves the rank and stores it in the memory location pointed to by the &rank\\n\\nvariable.\\n\\n2.\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &size) is an MPI function call which retrieves\\n\\nthe total number of processes in the communicator MPI_COMM_WORLD.\\n\\nParameters\\n\\n\\n\\n&size: This is the address of the variable where the total number of processes in the\\n\\ncommunicator will be stored. The function retrieves the size and stores it in the memory location\\n\\npointed to by the &size variable.\\n\\n3.\\n\\nMPI_Init(&argc, &argv) is an MPI (Message Passing Interface) function call used to\\n\\ninitialize the MPI environment. It is typically the first MPI function called in an MPI program.\\n\\nParameters\\n\\n&argc: This passes a pointer to the argc variable to the MPI library. The argc variable holds\\n\\nthe number of command-line arguments passed to the program.\\n\\n&argv: This passes a pointer to the argv variable to the MPI library. The argv variable is an\\n\\narray of strings containing the command-line arguments. When MPI initializes, it sets up\\n\\ncommunication channels between the processes, prepares the MPI environment for parallel\\n\\ncomputation\\n\\n4.\\n\\nMPI_Finalize() is an MPI (Message Passing Interface) function used to finalize the MPI\\n\\nenvironment. It is typically the last MPI function called in an MPI program, and it performs several\\n\\nimportant tasks to ensure the proper termination of the MPI application. MPI_Finalize() ensures\\n\\nthat all communication operations initiated by the program are completed before the program\\n\\nterminates.\\n\\nMPI Program Structure\\n\\nMPI Program\\n\\n#include <stdio.h>\\n\\n#include <mpi.h>\\n\\nint main(int argc, char** argv)\\n\\n{\\n\\nMPI_Init(&argc, &argv);\\n\\nint rank, size;\\n\\nMPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get the rank of the current process\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &size); // Get the total number of processes in\\n\\nthe communicator\\n\\nprintf(\"Hello from process %d of %d in MPI_COMM_WORLD\\\\n\", rank, size);\\n\\nMPI_Finalize();\\n\\nreturn 0;\\n\\n}\\n\\nMPI Datatypes: MPI provides its own reference data types corresponding to the various\\n\\nelementary data types in C.\\n\\nMPI Datatype\\n\\nMPI_CHAR\\n\\nsigned char\\n\\nMPI_SHORT\\n\\nsigned short int\\n\\nMPI_INT\\n\\nsigned int\\n\\nMPI_LONG\\n\\nsigned long int\\n\\nMPI_UNSIGNED_CHAR\\n\\nunsigned char\\n\\nMPI_UNSIGNED_SHORT\\n\\nunsigned short int\\n\\nMPI_UNSIGNED\\n\\nunsigned int\\n\\nMPI_UNSIGNED_LONG\\n\\nunsigned long int\\n\\nMPI_FLOAT\\n\\nfloat\\n\\nMPI_DOUBLE\\n\\ndouble\\n\\nMPI_LONG_DOUBLE\\n\\nlong double\\n\\nMPI_BYTE\\n\\n(none)\\n\\nMPI_PACKED\\n\\n(none)\\n\\nC Type\\n\\nUNIT-2\\n\\nTOPIC 4\\n\\nThe send and receive commands for process-to-process communication\\n\\nMPI Communication\\n\\nPoint to point Communication(Process to Process)\\n\\nCollective Communication\\n\\nBlocking Communication\\n\\nNon-Blocking Communication\\n\\nBarrier for Synchroniza tion\\n\\na\\n\\n\\uf0d8 Broadcast \\uf0d8 Gather, \\uf0d8 All gather \\uf0d8 Reduce, \\uf0d8 All Reduce \\uf0d8 Scatter\\n\\nThe core of the message-passing approach is to send a message from point-to-point or, perhaps\\n\\nmore precisely, process-to-process. The whole point of parallel processing is to coordinate work.\\n\\nThe Figure shows the three components for process to process communication\\n\\n\\uf0d8 Mail Box: There must be a mailbox at either end of the system. The size of the mailbox is\\n\\nimportant. The sending side knows the size of the message, but the receiving side does not.\\n\\nTo make sure there is a place for the message to be stored, it is usually better to post the\\n\\nreceive first. This avoids delaying the message by the receiving process having to allocate a\\n\\ntemporary space to store the message until a receive is posted and it can copy it to the right\\n\\nlocation. For an analogy, if the receive (mailbox) is not posted (not there), the postman has to\\n\\nhangout until someone puts one up. Posting the receive first avoids the possibility of\\n\\ninsufficient memory space on the receiving end to allocate a temporary buffer to store the\\n\\nmessage.\\n\\n\\uf0d8 Message :The message itself is always composed of a triplet at both ends: a pointer to a\\n\\nmemory buffer, a count, and a type. The type sent and type received can be different types\\n\\nand counts. The rationale for using types and counts is that it allows the conversion of types\\n\\nbetween the processes at the source and at the destination. This permits a message to be\\n\\nconverted to a different form at the receiving end. In a heterogeneous environment, this might\\n\\nmean converting lower-endian to big-endian, a lowlevel difference in the byte order of data\\n\\nstored on different hardware vendors. Also, the receive size can be greater than the amount\\n\\nsent. This permits the receiver to query how much data is sent so it can properly handle the\\n\\nmessage. But the receiving size cannot be smaller than the sending size because it would cause\\n\\na write past the end of the buffer.\\n\\n\\uf0d8 Envelope: The envelope also is composed of a triplet. It defines who the message is from, who it\\n\\nis sent to, and a message identifier to keep from getting multiple messages confused. The triplet\\n\\nconsists of the rank, tag, and communication group. The rank is for the specified communication\\n\\ngroup. The tag helps the programmer and MPI distinguish which message goes to which receive. In\\n\\nMPI, the tag is a convenience. It can be set to MPI_ANY_TAG if an explicit tag number is not desired.\\n\\nWe have two types of process to process communication: Blocking and non blocking\\n\\nBlocking communication in MPI refers to the type of communication where a process halts its\\n\\nexecution until a specific communication operation is completed. This means that the sending\\n\\nand receiving processes are synchronized also refered as Synchronuous Communication. The\\n\\nsender blocks until the receiver is ready to receive the message, and vice versa. The two most\\n\\ncommon blocking communication operations in MPI are MPI_Send and MPI_Recv.\\n\\nMPI_Send is a blocking communication function in MPI (Message Passing Interface) used for\\n\\nsending messages from one process to another. It sends a message from the sender process to the\\n\\nspecified destination process. Here is the syntax for MPI_Send\\n\\nMPI_Send(const void *buf, int count, MPI_Datatype datatype, int dest, int tag,\\n\\nMPI_Comm comm);\\n\\nbuf: A pointer to the send buffer (the data you want to send).\\n\\ncount: The number of elements in the send buffer.\\n\\ndatatype: The data type of the elements in the send buffer.\\n\\ndest: The rank of the destination process.\\n\\n\\n\\ntag: A message tag, which can be used by the receiver to distinguish different kinds of\\n\\nmessages.\\n\\ncomm: The communicator (usually MPI_COMM_WORLD for communication among all\\n\\nprocesses).\\n\\nMPI_Recv is a blocking communication function in MPI (Message Passing Interface) used for\\n\\nreceiving messages from other processes. It receives a message from a specified source process.\\n\\nHere is the syntax for MPI_Recv\\n\\nint MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm\\n\\ncomm, MPI_Status *status);\\n\\nbuf: A pointer to the receive buffer (where the received data will be stored).\\n\\ncount: The number of elements in the receive buffer.\\n\\ndatatype: The data type of the elements in the receive buffer.\\n\\n\\n\\nsource: The rank of the source process from which you want to receive the message. Use\\n\\nMPI_ANY_SOURCE if you want to receive a message from any source.\\n\\n\\n\\ntag: A message tag. If you used tags in MPI_Send, you can use the same tag here to filter\\n\\nmessages. MPI_ANY_TAG matches any tag comm\\n\\ncomm: The communicator (usually MPI_COMM_WORLD for communication among all\\n\\nprocesses).\\n\\n\\n\\nstatus: A pointer to an MPI_Status structure that will hold information about the received\\n\\nmessage, such as the source, tag, and error codes.\\n\\nProgram\\n\\n#include <mpi.h>\\n\\n#include <stdio.h>\\n\\nint main(int argc, char **argv) {\\n\\nint rank, size;\\n\\nint data_send = 42;\\n\\nint data_recv;\\n\\nMPI_Init(&argc, &argv);\\n\\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &size);\\n\\nif (size < 2) {\\n\\nprintf(\"This program requires at least 2 processes.\\\\n\");\\n\\nMPI_Finalize();\\n\\nreturn 1;\\n\\n}\\n\\n// Blocking Send from process 0 to process 1\\n\\nif (rank == 0) {\\n\\nMPI_Send(&data_send, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\\n\\nprintf(\"Process %d sent data: %d\\\\n\", rank, data_send);\\n\\n}\\n\\n// Blocking Receive at process 1\\n\\nelse if (rank == 1) {\\n\\nMPI_Recv(&data_recv, 1, MPI_INT, 0, 0, MPI_COMM_WORLD,\\n\\nMPI_STATUS_IGNORE);\\n\\nprintf(\"Process %d received data: %d\\\\n\", rank, data_recv);\\n\\n}\\n\\nMPI_Finalize();\\n\\nreturn 0;\\n\\n}\\n\\nProblems with blocking communication:\\n\\n\\uf0d8 A deadlock occurs when a set of processes are blocked because each is waiting for the other\\n\\nto release a resource. For example, if two processes are waiting for each other to send a\\n\\nmessage before they can receive, they will be deadlocked.\\n\\n\\uf0d8 Processes that are blocked waiting for communication can waste computational resources,\\n\\nsuch as CPU time and memory, as they are not performing useful work during that time.so\\n\\nwe go for non blocking communication\\n\\nNon blocking communication\\n\\nNon-blocking communication in MPI allows processes to initiate communication operations and\\n\\ncontinue their execution without waiting for the communication to complete. This is often referred\\n\\nto as asynchronous or non-blocking calls. Asynchronous means that the call initiates the operation\\n\\nbut does not wait for the completion of the work.\\n\\nMPI provides non-blocking communication functions like MPI_Isend( I means Immediate),\\n\\nMPI_Irecv, MPI_Test, MPI_Wait, and others to facilitate non-blocking communication.\\n\\nCompletion of a non-blocking send operation means that the sender is now free to update the\\n\\nsend buffer “message”.\\n\\nCompletion of a non-blocking receive operation means that the receive buffer “message”\\n\\ncontains the received data.\\n\\n\\uf0d8 MPI_Isend is a non-blocking communication function used for sending messages from\\n\\none process to another. Unlike MPI_Send, which is a blocking operation, MPI_Isend\\n\\nreturns immediately after initiating the send operation, allowing the sender process to\\n\\ncontinue its execution without waiting for the message to be delivered.\\n\\nMPI_Isend(const void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm\\n\\ncomm, MPI_Request *request);\\n\\nbuf: A pointer to the send buffer (the data you want to send).\\n\\ncount: The number of elements in the send buffer.\\n\\ndatatype: The data type of the elements in the send buffer.\\n\\ndest: The rank of the destination process.\\n\\n\\n\\ntag: A message tag, which can be used by the receiver to distinguish different kinds of\\n\\nmessages.\\n\\ncomm: The communicator (usually MPI_COMM_WORLD for communication among all\\n\\nprocesses).\\n\\nrequest: A pointer to an MPI_Request variable that will be used to identify the send request.\\n\\nYou can later use this request to check the status of the communication or wait for its\\n\\ncompletion.\\n\\n\\uf0d8 MPI_Irecv is a non-blocking communication function used for receiving messages from\\n\\nother processes. Unlike MPI_Recv, which is a blocking operation, MPI_Irecv returns\\n\\nimmediately after initiating the receive operation, allowing the receiving process to continue\\n\\nits execution without waiting for a message to arrive.\\n\\nMPI_Irecv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm\\n\\ncomm, MPI_Request *request);\\n\\nbuf: A pointer to the receive buffer (where the received data will be stored).\\n\\ncount: The number of elements in the receive buffer.\\n\\ndatatype: The data type of the elements in the receive buffer.\\n\\n\\n\\nsource: The rank of the source process from which you want to receive the message. Use\\n\\nMPI_ANY_SOURCE if you want to receive a message from any source.\\n\\n\\n\\ntag: A message tag. If you used tags in MPI_Send, you can use the same tag here to filter\\n\\nmessages.\\n\\ncomm: The communicator (usually MPI_COMM_WORLD for communication among all\\n\\nprocesses).\\n\\nrequest: A pointer to an MPI_Request variable that will be used to identify the receive\\n\\nrequest. You can later use this request to check the status of the communication or wait for its\\n\\ncompletion.\\n\\n\\uf0d8 MPI_Test is a non-blocking communication function used to check the completion status\\n\\nof a communication request initiated by non-blocking send (MPI_Isend) or receive\\n\\n(MPI_Irecv) operations. It allows you to query whether a non-blocking operation has been\\n\\ncompleted without waiting for its completion. Here is the syntax for MPI_Test:\\n\\nint MPI_Test(MPI_Request *request, int *flag, MPI_Status *status);\\n\\nrequest: A pointer to an MPI_Request variable that identifies the communication request.\\n\\n\\n\\nflag: A pointer to an integer variable that will be set to true (non-zero) if the communication\\n\\noperation associated with the request has completed, and false (zero) otherwise.\\n\\n\\n\\nstatus: A pointer to an MPI_Status structure that will hold information about the completed\\n\\ncommunication, such as the source, tag, and error codes.\\n\\nMPI_Test returns MPI_SUCCESS if the operation associated with the request has completed\\n\\nand flag is set to true. Otherwise, it returns MPI_ERR_PENDING if the operation is still\\n\\npending, meaning it has not yet completed.\\n\\n\\uf0d8\\n\\nMPI_Wait is a blocking function used to wait for the completion of a specific\\n\\ncommunication request, such as non-blocking send (MPI_Isend) or receive (MPI_Irecv)\\n\\noperations. It suspends the execution of the calling process until the specified communication\\n\\noperation is completed. Here is the syntax for MPI_Wait:\\n\\nint MPI_Wait(MPI_Request *request, MPI_Status *status);\\n\\nrequest: A pointer to an MPI_Request variable that identifies the communication request.\\n\\n\\n\\nstatus: A pointer to an MPI_Status structure that will hold information about the completed\\n\\ncommunication,\\n\\nsuch as\\n\\nthe\\n\\nsource,\\n\\ntag, and error codes. You can pass\\n\\nMPI_STATUS_IGNORE if you don\\'t need this information.\\n\\nMPI_Wait blocks until the communication associated with the specified request is complete. Once\\n\\nthe operation has completed, you can use the information in the status object if needed.\\n\\nProgram\\n\\n#include <mpi.h>\\n\\n#include <stdio.h>\\n\\nint main(int argc, char **argv) {\\n\\nint rank, size;\\n\\nint data_send = 42;\\n\\nint data_recv;\\n\\nMPI_Request request;\\n\\nMPI_Init(&argc, &argv);\\n\\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &size);\\n\\nif (size < 2) {\\n\\nprintf(\"This program requires at least 2 processes.\\\\n\");\\n\\nMPI_Finalize();\\n\\nreturn 1;\\n\\n}\\n\\n// Non-blocking Send from process 0 to process 1\\n\\nif (rank == 0) {\\n\\nMPI_Isend(&data_send, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);\\n\\nprintf(\"Process %d initiated non-blocking send with data: %d\\\\n\", rank, data_send);\\n\\n}\\n\\n// Non-blocking Receive at process 1\\n\\nelse if (rank == 1) {\\n\\nMPI_Irecv(&data_recv, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);\\n\\nprintf(\"Process %d initiated non-blocking receive.\\\\n\", rank);\\n\\n}\\n\\n// Wait for the non-blocking communication to complete\\n\\nMPI_Wait(&request, MPI_STATUS_IGNORE);\\n\\nif (rank == 1) {\\n\\nprintf(\"Process %d received data: %d\\\\n\", rank, data_recv);\\n\\n}\\n\\nMPI_Finalize();\\n\\nreturn 0;\\n\\n}\\n\\nAdvantages of Non Blocking Communication:\\n\\n1. Overlapping of Computation and Communication: Non-blocking operations allow\\n\\ncomputation and communication to occur concurrently. Processes can initiate communication\\n\\noperations and then continue with other computations without waiting for the communication to\\n\\ncomplete. This overlap of computation and communication can lead to improved overall\\n\\nperformance and better utilization of resources.\\n\\n2. Reduced Synchronization Overheads: Non-blocking operations reduce the need for\\n\\nsynchronization points in the code. With blocking communication, processes often need to\\n\\nsynchronize at communication points, leading to potential idle time for some processes. Non-\\n\\nblocking communication reduces these synchronization overheads and can lead to more balanced\\n\\nworkloads among processes.\\n\\n3. Minimized Potential for Deadlocks: Non-blocking communication reduces the likelihood\\n\\nof encountering deadlocks, which can occur in scenarios where processes are waiting for each other\\n\\nto release resources. Non-blocking operations allow processes to progress independently, reducing\\n\\nthe chances of deadlocks.\\n\\n4.\\n\\nBetter Load Balancing: Non-blocking operations enable dynamic load balancing techniques.\\n\\nProcesses can continue with computation tasks even when waiting for communication, allowing\\n\\nload balancing algorithms to adjust the workload dynamically based on the actual computational\\n\\nneeds of the processes.\\n\\n5. Optimized Network Utilization: Overlapping computation and communication can lead to\\n\\nmore efficient use of network resources. Processes can perform useful work while waiting for\\n\\nmessages, reducing idle time and maximizing the utilization of the communication network.\\n\\nOther variants of send/receive might be useful in special situations.\\n\\nThe modes are indicated by a one- or two-letter prefix, similar to that seen in the immediate\\n\\nvariant, as listed here:\\n\\nB (buffered)\\n\\nS (synchronous)\\n\\nR (ready)\\n\\nIB (immediate buffered)\\n\\nIS (immediate synchronous)\\n\\nIR (immediate ready)\\n\\nUNIT-2\\n\\nTOPIC 5\\n\\nCollective communication\\n\\nCollective communication refers to a type of communication pattern in parallel and distributed\\n\\ncomputing, where multiple processes or nodes collaborate to exchange information among\\n\\nthemselves. These communication patterns are essential in high-performance computing and\\n\\ndistributed systems to efficiently solve problems that require coordinated efforts among multiple\\n\\nparticipants.\\n\\nTypes of collective communication\\n\\nCollective communication operations are made of the following types:\\n\\n1. Barrier: A barrier is a synchronization point that forces all processes in a group to wait until\\n\\nthey have all reached the barrier before continuing. Barriers are often used to ensure that all\\n\\nprocesses are at the same point in their execution.\\n\\nSyntax:\\n\\nint MPI_Barrier(MPI_Comm communicator);\\n\\ncommunicator: The communicator that defines the group of processes that synchronize at the barrier.\\n\\nThe MPI_Barrier function is often used to coordinate the execution of processes in a parallel\\n\\nprogram. For example, if different processes are performing different parts of a computation and need\\n\\nto ensure that they all reach a certain point before proceeding.\\n\\nProgram\\n\\n#include <mpi.h>\\n\\n#include <stdio.h>\\n\\nint main(int argc, char** argv) {\\n\\nint rank, size;\\n\\nMPI_Init(&argc, &argv);\\n\\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &size);\\n\\n// Some computation before the barrier\\n\\nprintf(\"Process %d reached the barrier.\\\\n\", rank);\\n\\nMPI_Barrier(MPI_COMM_WORLD); // All processes wait here until everyone reaches this point\\n\\n// Code after the barrier\\n\\nMPI_Finalize();\\n\\nreturn 0;\\n\\n}\\n\\n2. Data Movement (or Global Communication):\\n\\nBroadcast: In a broadcast operation, one process sends the same data to all other\\n\\nprocesses in a group. It is often used to distribute information from one process to all\\n\\nothers.\\n\\nSyntax\\n\\nMPI_Bcast( void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator)\\n\\n\\n\\ndata: A pointer to the data that the root process wants to broadcast. This data is sent by the\\n\\nroot process and received by all other processes.\\n\\n\\n\\ncount: The number of data elements in the buffer.\\n\\n\\n\\ndatatype: The datatype of the elements in the buffer.\\n\\n\\n\\nroot: The rank of the process within the communicator that is broadcasting the data.\\n\\n\\n\\ncommunicator: The communicator that defines the group of processes over which the\\n\\nbroadcast operation is performed.\\n\\nProgram\\n\\n#include<stdio.h>\\n\\n#include<mpi.h>\\n\\nint main(int argc, char* argv[])\\n\\n{\\n\\nint a = 10, r, s;\\n\\nMPI_Init(NULL, NULL);\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &s);\\n\\nMPI_Comm_rank(MPI_COMM_WORLD, &r);\\n\\nprintf(\"\\\\n data in process %d before bcast=%d\", r, a);\\n\\nMPI_Bcast(&a, 1, MPI_INT, 0, MPI_COMM_WORLD);\\n\\nprintf(\"\\\\n data in process %d after bcast=%d\",r,a);\\n\\nMPI_Finalize(); return 0;\\n\\n}\\n\\nOutput\\n\\n\\n\\nGather: The gather operation collects data from all processes in a group and sends it to a\\n\\ndesignated process. This is useful when you want to aggregate data from multiple sources.\\n\\nSyntax\\n\\nMPI_Gather( void* sendbuf, int send_count, MPI_Datatype sendtype, void * recvbuf, int\\n\\nrecvcount, MPI_Datatype recvtype, int root, MPI_Comm communicator)\\n\\nsendbuf: A pointer to the send buffer (data to be sent) on each process.\\n\\nsendcount: The number of elements to send from the send buffer.\\n\\nsendtype: The datatype of the elements in the send buffer.\\n\\nrecvbuf: A pointer to the receive buffer on the root process. This is where the gathered data will\\n\\nbe stored.\\n\\nrecvcount: The number of elements to receive from each process.\\n\\nrecvtype: The datatype of the elements in the receive buffer.\\n\\nroot: The rank of the root process, which will receive the gathered data.\\n\\ncommunicator: The communicator that defines the group of processes.\\n\\nProgram\\n\\n#include<stdio.h>\\n\\n#include<mpi.h>\\n\\nint main(int argc, char* argv[])\\n\\n{\\n\\nint d = 0, r, s, a[5];\\n\\nMPI_Init(NULL, NULL);\\n\\nMPI_Comm_size(MPI_COMM_WORLD,&s); MPI_Comm_rank(MPI_COMM_WORLD, &r);\\n\\nd = r * 2;\\n\\nMPI_Gather(&d, 1, MPI_INT, &a, 1, MPI_INT, 0, MPI_COMM_WORLD);\\n\\nif (r == 0)\\n\\n{\\n\\nprintf(\"data received by process o=\");\\n\\nfor (int i = 0;i < s;i++)\\n\\nprintf(\"%d\\\\t\", a[i]);\\n\\n}\\n\\nMPI_Finalize(); return 0;\\n\\n}\\n\\nAllgather : MPI_Allgather distributes the gathered data to all processes in the\\n\\ncommunicator, not just to the root process. Each process receives the entire gathered dataset.\\n\\nSyntax\\n\\nMPI_Allgather( void* sendbuf, int sendcount, MPI_Datatype senddatatype, void* recvbuf, int\\n\\nrecvcount, MPI_Datatype recvtype, MPI_Comm communicator)\\n\\nsendbuf: A pointer to the send buffer (data to be sent) on each process.\\n\\nsendcount: The number of elements to send from the send buffer on each process.\\n\\nsendtype: The datatype of the elements in the send buffer.\\n\\nrecvbuf: A pointer to the receive buffer on each process. This is where the gathered data will\\n\\nbe stored.\\n\\nrecvcount: The number of elements to receive from each process.\\n\\nrecvtype: The datatype of the elements in the receive buffer.\\n\\ncommunicator: The communicator that defines the group of processes\\n\\nHere\\'s how MPI_Allgather works:\\n\\nEach process provides data in its send buffer (sendbuf).\\n\\nThe data from the send buffers of all processes is gathered.\\n\\nThe gathered data is distributed to the receive buffers of all processes (recvbuf).\\n\\nProgram\\n\\n#include<stdio.h>\\n\\n#include<mpi.h>\\n\\nint main(int argc, char* argv[])\\n\\n{\\n\\nint d = 0, r, s, a[5];\\n\\nMPI_Init(NULL, NULL);\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &s); MPI_Comm_rank(MPI_COMM_WORLD,\\n\\n&r);\\n\\nd = r * 2;\\n\\nMPI_Allgather (&d, 1, MPI_INT, &a, 1, MPI_INT, MPI_COMM_WORLD);\\n\\nprintf(\"data received by process %d=\",r);\\n\\nfor (int i = 0;i < s;i++)\\n\\nprintf(\"%d\\\\t\", a[i]);\\n\\nMPI_Finalize(); return 0;\\n\\n}\\n\\nReduce: In a reduce operation, data from all processes is combined using an associative and\\n\\ncommutative operation (e.g., addition, multiplication) to produce a single result. This is often used\\n\\nfor aggregating data or finding global statistics. There are many operations that can be done during\\n\\nthe reduction. The most common are\\n\\nMPI_MAX (maximum value in an array)\\n\\nMPI_MIN (minimum value in an array)\\n\\nMPI_SUM (sum of an array)\\n\\nMPI_MINLOC (index of minimum value)\\n\\nMPI_MAXLOC (index of maximum value\\n\\nExample:\\n\\nSyntax\\n\\nMPI_Reduce( void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op,\\n\\nint root, MPI_Comm communicator)\\n\\nsendbuf: A pointer to the send buffer (data to be reduced) on each process.\\n\\nrecvbuf: A pointer to the receive buffer on the root process. This is where the reduced result\\n\\nwill be stored.\\n\\ncount: The number of elements in the send buffer.\\n\\ndatatype: The datatype of the elements in the send buffer.\\n\\nop: The reduction operation to be performed (e.g., MPI_SUM, MPI_MAX, MPI_MIN,\\n\\nMPI_PROD,etc.).\\n\\nroot: The rank of the root process, where the reduced result will be stored.\\n\\ncommunicator: The communicator that defines the group of processes\\n\\nProgram\\n\\nint main(int argc, char* argv[])\\n\\n{\\n\\nint size, rank;\\n\\nMPI_Init(NULL,NULL);\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &size); MPI_Comm_rank(MPI_COMM_WORLD,\\n\\n&rank);\\n\\nint localsum ;\\n\\nint globalsum;\\n\\nlocalsum = 10 + rank;\\n\\nprintf(\"process %d value=%d\", rank, localsum);\\n\\nMPI_Reduce(&localsum, &globalsum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\\n\\nif (rank == 0)\\n\\n{\\n\\nprintf(\"\\\\n globalsum = %d\", globalsum);\\n\\n}\\n\\nMPI_Finalize();\\n\\nreturn (0);\\n\\n}\\n\\nOutput for 3 processes: global sum=13\\n\\nFirst process local sum=10+0(rank)=10\\n\\nSecond process local sum=10+1(rank)=11\\n\\nThird process local sum=11+2(rank)=13\\n\\n\\n\\nAllReduce: MPI_Allreduce is a collective communication function in the MPI (Message\\n\\nPassing Interface) standard. It is similar to MPI_Reduce in that it performs a reduction operation\\n\\nacross all processes in a communicator. However, unlike MPI_Reduce, the result of the reduction\\n\\noperation is distributed to all processes, not just the root process. Every process receives the reduced\\n\\nresult.\\n\\nExample\\n\\nSyntax\\n\\n\\n\\nMPI_Allreduce( void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype,\\n\\nMPI_Op op, MPI_Comm communicator);\\n\\nsendbuf: A pointer to the send buffer (data to be reduced) on each process.\\n\\nrecvbuf: A pointer to the receive buffer on each process. This is where the reduced result will\\n\\nbe stored.\\n\\ncount: The number of elements in the send buffer.\\n\\ndatatype: The datatype of the elements in the send buffer.\\n\\nop: The reduction operation to be performed (e.g., MPI_SUM, MPI_MAX, MPI_MIN, MPI_PROD,\\n\\nMPI_LAND, MPI_BAND, MPI_LOR, etc.).\\n\\ncommunicator: The communicator that defines the group of processes.\\n\\nProgram\\n\\n#include <stdio.h>\\n\\n#include <stdlib.h>\\n\\n#include <mpi.h>\\n\\nint main(int argc, char* argv[])\\n\\n{\\n\\nint size, rank;\\n\\nMPI_Init(NULL, NULL);\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &size); MPI_Comm_rank(MPI_COMM_WORLD,\\n\\n&rank);\\n\\nint globalsum;\\n\\nprintf(\"process %d value=%d\", rank, localsum);\\n\\nMPI_Allreduce(&rank, &globalsum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\\n\\nprintf(\"\\\\n globalsum = %d\", globalsum);\\n\\nMPI_Finalize();\\n\\nreturn (0);\\n\\n}\\n\\nOutput for 3 processes: global sum=13 ,3 times for 3 process\\n\\nScatter: Scatter is the opposite of gather. It takes data from one process and distributes it to\\n\\nall other processes in a group. Each process receives a different portion of the data.\\n\\nSyntax\\n\\nMPI_Scatter( void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount,\\n\\nMPI_Datatype recvtype, int root, MPI_Comm communicator);\\n\\nsendbuf: A pointer to the send buffer (data to be scattered) on the root process.\\n\\nsendcount: The number of elements to send from the send buffer on the root process.\\n\\nsendtype: The datatype of the elements in the send buffer.\\n\\nrecvbuf: A pointer to the receive buffer on each process. This is where the scattered data will be\\n\\nstored.\\n\\nrecvcount: The number of elements to receive on each process.\\n\\nrecvtype: The datatype of the elements in the receive buffer.\\n\\nroot: The rank of the root process, which is the source of the scattered data.\\n\\ncommunicator: The communicator that defines the group of processes.\\n\\nProgram\\n\\n#include<stdio.h>\\n\\n#include<mpi.h>\\n\\nint main(int argc, char* argv[])\\n\\n{\\n\\nint d = 0, r, s, * buf=NULL; MPI_Init(NULL, NULL);\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &s); MPI_Comm_rank(MPI_COMM_WORLD, &r);\\n\\nif(r == 0)\\n\\n{\\n\\nint a[5] = { 1,2,3,4,5 };\\n\\nbuf = a;\\n\\n}\\n\\nprintf(\"\\\\n data in process %d before scatter=%d\", r, d);\\n\\nMPI_Scatter(buf, 1, MPI_INT, &d, 1, MPI_INT, 0, MPI_COMM_WORLD);\\n\\nprintf(\"\\\\n data in process %d after scatter=%d\", r, d); MPI_Finalize();\\n\\nreturn 0;\\n\\n}\\n\\nOutput\\n\\nUNIT-2\\n\\nTOPIC 6\\n\\nData Parallel Examples\\n\\nThe data parallel strategy is the most common approach in parallel applications.\\n\\nFirst, a simple case of the stream triad where no communication is necessary.\\n\\nThe Stream Triad benchmark measures the memory bandwidth of a computing system. It is a simple\\n\\nyet effective benchmark to assess the memory performance of a node. The Stream Triad benchmark\\n\\ncalculates the memory bandwidth by performing a simple operation on arrays in memory.\\n\\nExample\\n\\n#include <stdio.h>\\n\\n#include <stdlib.h>\\n\\n#include <omp.h>\\n\\n#define ARRAY_SIZE (1 << 20) // 1 million elements (adjust based on your system)\\n\\n#define SCALAR 2.0\\n\\nint main() {\\n\\ndouble *a, *b, *c;\\n\\nint i;\\n\\ndouble start_time, end_time;\\n\\ndouble bandwidth;\\n\\n// Allocate memory for arrays\\n\\na = (double*)malloc(ARRAY_SIZE * sizeof(double));\\n\\nb = (double*)malloc(ARRAY_SIZE * sizeof(double));\\n\\nc = (double*)malloc(ARRAY_SIZE * sizeof(double));\\n\\n// Initialize arrays\\n\\n#pragma omp parallel for\\n\\nfor (i = 0; i < ARRAY_SIZE; i++) {\\n\\na[i] = 1.0;\\n\\nb[i] = 2.0;\\n\\nc[i] = 0.0;\\n\\n}\\n\\n// Measure start time\\n\\nstart_time = omp_get_wtime();\\n\\n// Perform Stream Triad operation\\n\\n#pragma omp parallel for\\n\\nfor (i = 0; i < ARRAY_SIZE; i++) {\\n\\nc[i] = a[i] + b[i] * SCALAR;\\n\\n}\\n\\n// Measure end time\\n\\nend_time = omp_get_wtime();\\n\\n// Calculate bandwidth in GB/s\\n\\nbandwidth = (3 * ARRAY_SIZE * sizeof(double)) / ((end_time - start_time) * 1e9);\\n\\n// Print bandwidth\\n\\nprintf(\"Memory Bandwidth: %f GB/s\\\\n\", bandwidth);\\n\\n// Free allocated memory\\n\\nfree(a);\\n\\nfree(b);\\n\\nfree(c);\\n\\nreturn 0;\\n\\n}\\n\\nSecond the more typical ghost cell exchange techniques used to link together the subdivided\\n\\ndomains distributed to each process\\n\\nGhost cell exchange/updates are a common technique in computational science and high-\\n\\nperformance computing, particularly in the context of numerical simulations, to manage boundary\\n\\nconditions and ensure accurate results when performing computations on a grid or mesh. Ghost\\n\\ncells, also known as halo cells or boundary cells, are additional grid cells that surround the main\\n\\ncomputational domain.\\n\\nThe update of ghost cells involves exchanging data between neighbouring chunks to ensure that\\n\\neach chunk has the correct information about its boundary cells. This communication is necessary\\n\\nbecause the computation in one chunk often depends on the values of neighbouring cells.\\n\\nThe white cells represeent the halo cells which can be used for temporary storage of values while\\n\\nexchanging the data.Each mesh is assigned to different processors for performing operations.\\n\\nWe can see how the cells are exchanged from one mesh to another with the help of halo\\n\\ncells.(Colors are swapped).\\n\\nAdvanced MPI functionality to simplify code and enable optimizations\\n\\nThe advanced functions that are useful in common data parallel applications.\\n\\nMPI custom data types\\n\\nTopology support\\n\\nCustom MPI Data Types:\\n\\nMPI has a rich set of functions to create new, custom MPI data types from the basic MPI types\\n\\n\\uf0d8 MPI_Type_contiguous—makes a block of contiguous data into a type.\\n\\n\\uf0d8 MPI_Type_vector—creates a type out of blocks of strided data. (elements in strided\\n\\ndata are not necessarily contiguous in memory; there are gaps between them).\\n\\n\\uf0d8 MPI_Type_create_subarray—creates a rectangular subset of a larger array.\\n\\n\\uf0d8 MPI_Type_create_struct—Creates a data type encapsulating the data items in a\\n\\nstructure in a portable way that accounts for padding by the compiler\\n\\nThree MPI custom data types with illustrations of the arguments used in their creation\\n\\nA type must be committed before use and it must be freed to avoid a memory leak. The routines\\n\\ninclude\\n\\n\\uf0d8 MPI_Type_Commit—Initializes the new custom type with needed memory allocation or\\n\\nother setup\\n\\n\\uf0d8 MPI_Type_Free—Frees any memory or data structure entries from the creation of the data\\n\\ntype\\n\\nTopology support\\n\\nCartesian topology support in MPI (Message Passing Interface) allows you to define a logical, multi-\\n\\ndimensional grid or mesh of processes to facilitate communication and coordination in parallel\\n\\napplications. This is particularly useful for simulations, numerical computations, and other scientific\\n\\ncomputing tasks where data is organized in multi-dimensional arrays or grids.\\n\\nTo create a Cartesian topology in MPI, you typically follow these steps:\\n\\n1. Initialize MPI and determine the size and rank of your MPI communicator.\\n\\n#include <mpi.h>\\n\\nint main(int argc, char** argv) {\\n\\nMPI_Init(&argc, &argv);\\n\\nint size, rank;\\n\\nMPI_Comm_size(MPI_COMM_WORLD, &size);\\n\\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\\n\\nMPI_Finalize();\\n\\nreturn 0;\\n\\n}\\n\\n2. Define the dimensions and periodicity of the grid using an integer array, and create the\\n\\nCartesian communicator using MPI_Cart_create\\n\\nint dims[ndims]; // Array specifying the number of processes in each dimension\\n\\nint periods[ndims]; // Array indicating whether the grid is periodic in each dimension\\n\\nMPI_Comm cart_comm;\\n\\nMPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 0, &cart_comm);\\n\\nParameters:\\n\\nMPI_COMM_WORLD:This is the communicator representing all the processes that are\\n\\ninvolved in the original communication world.\\n\\nndims:An integer representing the number of dimensions of the Cartesian grid. For example,\\n\\nndims = 2 for a 2D grid (like a matrix) or ndims = 3 for a 3D grid.\\n\\ndims[]:An array of integers specifying the number of processes in each dimension. For example,\\n\\nfor a 2D grid with 4 processes in each dimension, dims = {4, 4} would mean a grid with 16\\n\\nprocesses in total.\\n\\nperiods[]:An array of integers of size ndims that specifies whether the grid should be periodic in\\n\\neach dimension.\\n\\n1 indicates periodic (like wrapping around, torus-like),\\n\\n0 indicates non-periodic (no wrapping).\\n\\n0(reorderflag):This flag indicates whether process ranks should be reordered to optimize the\\n\\ntopology.\\n\\nIf set to 1, processes may be reordered for performance reasons.\\n\\nIf set to 0, the rank assignment remains unchanged.\\n\\n&cart_comm:A pointer to the new communicator that will be created. This new communicator\\n\\nwill include the processes arranged in the specified Cartesian topology. cart_comm will be used\\n\\nin future communication within this topology.\\n\\n3. Retrieve the coordinates of each process in the Cartesian grid using MPI_Cart_coords.\\n\\nint coords[ndims];\\n\\nMPI_Cart_coords(cart_comm, rank, ndims, coords);\\n\\nParameters:\\n\\ncomm:The communicator with Cartesian structure (the communicator created by\\n\\nMPI_Cart_create).\\n\\nrank:The rank of the process whose coordinates you want to determine (within the\\n\\nCartesian communicator).\\n\\nmaxdims:The number of dimensions in the Cartesian grid (same as ndims passed to\\n\\nMPI_Cart_create).\\n\\ncoords[]:An integer array that will hold the coordinates of the process. The array should\\n\\nbe of size maxdims to hold the Cartesian coordinates in each dimension.\\n\\nReturn Value:\\n\\nThe function returns an array of integers (coords) containing the coordinates of the process\\n\\nin the Cartesian topology. The return value is MPI_SUCCESS if the function completes\\n\\nsuccessfully.\\n\\n4. Perform point-to-point communication or collective operations specific to your application\\'s\\n\\ngrid structure, often using functions like MPI_Send, MPI_Recv, and collective operations like\\n\\nMPI_Allreduce, MPI_Gather, or MPI_Scatter.\\n\\n5. After you\\'re done, free the Cartesian communicator using MPI_Comm_free\\n\\nMPI_Comm_free(&cart_comm);')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document chunks\n",
    "text_splitter = CharacterTextSplitter(separator=\"/n\",\n",
    "                                      chunk_size=7500,\n",
    "                                      chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srujana\\AppData\\Local\\Temp\\ipykernel_16956\\3655315981.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "c:\\Users\\Srujana\\anaconda3\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\Srujana\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = FAISS.from_documents(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=knowledge_base.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions =input(\"Enter Noof Questions:\")\n",
    "Type = input(\"Enter the type(Mcqs/True or false/Fill in the blanks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True or false'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Questions\n",
    "Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate 5 True or false'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prompt = f\"Generate {Questions} {Type}\"\n",
    "Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are five true or false statements related to Cartesian topology in MPI:\n",
      "\n",
      "1. True: In Cartesian topology, each process has a unique rank within the communicator. (True)\n",
      "2. False: The number of dimensions in a Cartesian grid is fixed and cannot be changed dynamically. (False - you can modify the number of dimensions at runtime using MPI_Cart_create with a non-zero value for ndims)\n",
      "3. True: In a Cartesian topology, processes are arranged in a linear or one-dimensional structure. (True)\n",
      "4. False: The MPI_Cart_coords function returns the coordinates of each process in the Cartesian grid. (False - it returns an integer array containing the coordinates of the process in the specified dimension(s))\n",
      "5. True: In a Cartesian topology, communication between processes is always point-to-point. (False - you can perform collective operations like MPI_Allreduce or MPI_Gather on groups of processes)\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.invoke({\"query\": Prompt})\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  This document appears to be a guide on how to use the Message Passing Interface (MPI) for parallel computing, specifically focusing on Cartesian topology support in MPI. The document covers the following topics:\n",
      "\n",
      "1. Initializing MPI and determining the size and rank of processes in a communicator.\n",
      "2. Creating a Cartesian communicator using MPI_Cart_create() and defining the dimensions and periodicity of the grid.\n",
      "3. Retrieving the coordinates of each process in the Cartesian grid using MPI_Cart_coords().\n",
      "4. Performing point-to-point communication or collective operations specific to your application's grid structure, such as MPI_Send, MPI_Recv, and collective operations like MPI_Allreduce, MPI_Gather, or MPI_Scatter.\n",
      "5. Freeing the Cartesian communicator after use using MPI_Comm_free().\n"
     ]
    }
   ],
   "source": [
    "question = \"What is this document about?\"\n",
    "response = qa_chain.invoke({\"query\": question})\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are five MCQs related to vectorization methods, along with their answers:\n",
      "\n",
      "MCQ1: What is the primary advantage of using vectorization techniques in programming?\n",
      "A. Improved code readability\n",
      "B. Faster execution time\n",
      "C. Simplified data manipulation\n",
      "D. Better memory management\n",
      "Answer: B. Faster execution time\n",
      "\n",
      "MCQ2: Which of the following vectorization methods is most suitable for performing matrix operations?\n",
      "A. Recursion\n",
      "B. Iteration\n",
      "C. Looping\n",
      "D. Function calls\n",
      "Answer: C. Looping\n",
      "\n",
      "MCQ3: What is the purpose of using a hash table in vectorization?\n",
      "A. To store data in a sorted order\n",
      "B. To reduce memory usage by storing data in an array\n",
      "C. To perform fast lookups for elements in a large dataset\n",
      "D. To implement recursion in algorithms\n",
      "Answer: C. To perform fast lookups for elements in a large dataset\n",
      "\n",
      "MCQ4: Which of the following vectorization techniques is most efficient for searching an element in a sorted array?\n",
      "A. Linear search\n",
      "B. Binary search\n",
      "C. Iteration through all elements\n",
      "D. Recursion\n",
      "Answer: B. Binary search\n",
      "\n",
      "MCQ5: What is the main advantage of using a recursive function to solve a problem?\n",
      "A. It reduces memory usage by avoiding arrays\n",
      "B. It improves code readability and maintainability\n",
      "C. It allows for faster execution time by avoiding loops\n",
      "D. It simplifies data manipulation by avoiding complex operations\n",
      "Answer: C. It allows for faster execution time by avoiding loops\n"
     ]
    }
   ],
   "source": [
    "question = \"generate 5 mcqs on Vectorization methods with answers\"\n",
    "response = qa_chain.invoke({\"query\": question})\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are 10 true or false questions related to vectorization methods in parallel computing, along with their answers:\n",
      "\n",
      "1. True or False: The Parallel Random Access (PRA) method is a vectorization technique that involves dividing the data into smaller chunks and processing them in parallel. (True)\n",
      "2. True or False: The Data Dependence Graph (DDG) is a graphical representation of the dependencies between variables in a program, which can be used to identify potential parallelism. (False - DDG is actually a technique for analyzing the data dependencies in a program to identify opportunities for parallelization.)\n",
      "3. True or False: The OpenMP compiler generates optimized code by automatically vectorizing loops. (True)\n",
      "4. True or False: The MPI_Allreduce() function in MPI is used for collective operations such as summing the values of all processors. (False - MPI_Allreduce() is actually a function for reducing the values of all processors.)\n",
      "5. True or False: The OpenMP directives #PARALLEL and #DATA_SHARED are used to specify the number of threads that can execute simultaneously. (True)\n",
      "6. True or False: The MPI_Barrier() function in MPI is used to synchronize all processes in a communicator before executing any further operations. (False - MPI_Barrier() is actually a function for blocking all processes until they are ready to proceed.)\n",
      "7. True or False: The OpenMP _omp parallel directive can be used to specify the number of threads that should execute simultaneously. (True)\n",
      "8. True or False: The MPI_Reduce() function in MPI is used for collective operations such as summing the values of all processors. (False - MPI_Reduce() is actually a function for reducing the values of all processors.)\n",
      "9. True or False: The OpenMP _omp critical directive can be used to specify the number of threads that should execute simultaneously. (True)\n",
      "10. True or False: The MPI_Gather() function in MPI is used for collective operations such as summing the values of all processors. (False - MPI_Gather() is actually a function for gathering data from all processes.)\n"
     ]
    }
   ],
   "source": [
    "'''question = \"generate 10 true/false questions  with answers on Vectorization methods with answers\"\n",
    "response = qa_chain.invoke({\"query\": question})\n",
    "print(response[\"result\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving PDF in directory: c:\\Users\\Srujana\\OneDrive\\Desktop\\MODEL\n",
      "Response saved to c:\\Users\\Srujana\\OneDrive\\Desktop\\MODEL\\True or false_5_351.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms import Ollama\n",
    "from fpdf import FPDF\n",
    "import os\n",
    "\n",
    "# Function to save response to a PDF\n",
    "def save_to_pdf(response, output_file):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    pdf.multi_cell(0, 10, response)\n",
    "    pdf.output(output_file)\n",
    "    print(f\"Response saved to {output_file}\")\n",
    "\n",
    "# RAG Setup and PDF Saving\n",
    "def main():\n",
    "    # Example response for testing purposes\n",
    "    # In a real scenario, you will be working with your RAG pipeline to get this result.\n",
    "    result = response[\"result\"]\n",
    "\n",
    "    # Get current working directory to save the output PDF\n",
    "    current_directory = os.getcwd()\n",
    "    print(f\"Saving PDF in directory: {current_directory}\")\n",
    "\n",
    "    x = random.randint(1, 1000)  # Use a larger range to reduce conflicts\n",
    "    output_file = os.path.join(current_directory, f\"{Type}_{Questions}_{x}.pdf\")\n",
    "    # Save result to PDF\n",
    "    save_to_pdf(result, output_file)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
